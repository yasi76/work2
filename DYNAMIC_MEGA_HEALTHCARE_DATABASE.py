#!/usr/bin/env python3
"""
üè• Dynamic MEGA European Healthcare Database Builder
üöÄ Uses URLs from Dynamic Research Discovery + Your Manual URLs
===============================================================
This script loads URLs from the dynamic research discovery process
and combines them with your manual URLs for comprehensive validation.
"""

import urllib.request
import urllib.parse
import urllib.error
import csv
import json
import time
import re
import ssl
import os
import glob
from datetime import datetime
from urllib.parse import urlparse, urljoin

# Your Original Manual URLs (always preserved)
MANUAL_URLS = [
    'https://www.acalta.de',
    'https://www.actimi.com',
    'https://www.emmora.de',
    'https://www.alfa-ai.com',
    'https://www.apheris.com',
    'https://www.aporize.com/',
    'https://www.arztlena.com/',
    'https://shop.getnutrio.com/',
    'https://www.auta.health/',
    'https://visioncheckout.com/',
    'https://www.avayl.tech/',
    'https://www.avimedical.com/avi-impact',
    'https://de.becureglobal.com/',
    'https://bellehealth.co/de/',
    'https://www.biotx.ai/',
    'https://www.brainjo.de/',
    'https://brea.app/',
    'https://breathment.com/',
    'https://de.caona.eu/',
    'https://www.careanimations.de/',
    'https://sfs-healthcare.com',
    'https://www.climedo.de/',
    'https://www.cliniserve.de/',
    'https://cogthera.de/#erfahren',
    'https://www.comuny.de/',
    'https://curecurve.de/elina-app/',
    'https://www.cynteract.com/de/rehabilitation',
    'https://www.healthmeapp.de/de/',
    'https://deepeye.ai/',
    'https://www.deepmentation.ai/',
    'https://denton-systems.de/',
    'https://www.derma2go.com/',
    'https://www.dianovi.com/',
    'http://dopavision.com/',
    'https://www.dpv-analytics.com/',
    'http://www.ecovery.de/',
    'https://elixionmedical.com/',
    'https://www.empident.de/',
    'https://eye2you.ai/',
    'https://www.fitwhit.de',
    'https://www.floy.com/',
    'https://fyzo.de/assistant/',
    'https://www.gesund.de/app',
    'https://www.glaice.de/',
    'https://gleea.de/',
    'https://www.guidecare.de/',
    'https://www.apodienste.com/',
    'https://www.help-app.de/',
    'https://www.heynanny.com/',
    'https://incontalert.de/',
    'https://home.informme.info/',
    'https://www.kranushealth.com/de/therapien/haeufiger-harndrang',
    'https://www.kranushealth.com/de/therapien/inkontinenz'
]

def load_discovered_urls():
    """
    Load URLs from the dynamic research discovery script
    """
    print("üîç Loading URLs from Dynamic Research Discovery...")
    
    discovered_urls = []
    
    # Look for the most recent URL files
    url_files = glob.glob("SIMPLE_URL_LIST_*.txt")
    json_files = glob.glob("DISCOVERED_URLS_FOR_MEGA_*.json")
    
    if url_files:
        # Use the most recent simple URL list
        latest_url_file = max(url_files, key=os.path.getctime)
        print(f"üìÇ Found URL file: {latest_url_file}")
        
        try:
            with open(latest_url_file, 'r', encoding='utf-8') as f:
                discovered_urls = [line.strip() for line in f if line.strip()]
            print(f"‚úÖ Loaded {len(discovered_urls)} URLs from {latest_url_file}")
        except Exception as e:
            print(f"‚ùå Error loading {latest_url_file}: {e}")
    
    elif json_files:
        # Use the most recent JSON file if no simple file found
        latest_json_file = max(json_files, key=os.path.getctime)
        print(f"üìÇ Found JSON file: {latest_json_file}")
        
        try:
            with open(latest_json_file, 'r', encoding='utf-8') as f:
                url_data = json.load(f)
                discovered_urls = [item['url'] for item in url_data if 'url' in item]
            print(f"‚úÖ Loaded {len(discovered_urls)} URLs from {latest_json_file}")
        except Exception as e:
            print(f"‚ùå Error loading {latest_json_file}: {e}")
    
    else:
        print("‚ö†Ô∏è No discovered URL files found.")
        print("üí° Run DYNAMIC_RESEARCH_DISCOVERY.py first to generate URLs.")
        discovered_urls = []
    
    return discovered_urls

def create_safe_request(url, timeout=15):
    """Create a safe HTTP request with error handling"""
    try:
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        request = urllib.request.Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
        )
        
        response = urllib.request.urlopen(request, context=ssl_context, timeout=timeout)
        content = response.read()
        
        encoding = response.headers.get_content_charset() or 'utf-8'
        try:
            decoded_content = content.decode(encoding, errors='ignore')
        except:
            decoded_content = content.decode('utf-8', errors='ignore')
            
        return decoded_content, response.getcode()
    except Exception as e:
        return None, str(e)

def clean_and_deduplicate_urls(discovered_urls, manual_urls):
    """Clean and deduplicate all URLs"""
    print("\nüßπ Combining and Deduplicating URLs...")
    
    all_urls = set()
    
    # Add manual URLs
    for url in manual_urls:
        clean_url = url.strip().rstrip('/')
        if clean_url:
            all_urls.add(clean_url)
    
    # Add discovered URLs
    for url in discovered_urls:
        clean_url = url.strip().rstrip('/')
        if clean_url and is_company_url(clean_url):
            all_urls.add(clean_url)
    
    # Convert back to list and sort
    final_urls = sorted(list(all_urls))
    
    print(f"üìä Manual URLs: {len(manual_urls)}")
    print(f"üìä Discovered URLs: {len(discovered_urls)}")
    print(f"üìä Total unique URLs: {len(final_urls)}")
    print(f"üìä Dynamic discoveries: {len(final_urls) - len(manual_urls)}")
    
    return final_urls

def is_company_url(url):
    """Filter out non-company URLs"""
    url_lower = url.lower()
    
    # Exclude patterns
    exclude_patterns = [
        'facebook.com', 'twitter.com', 'linkedin.com', 'youtube.com', 'instagram.com',
        'google.com', 'wikipedia.org', 'github.com', 'stackoverflow.com',
        'news', 'blog', 'forum', 'discussion', 'comment', 'article',
        'privacy', 'terms', 'legal', 'contact', 'about-us', 'imprint',
        '.pdf', '.doc', '.jpg', '.png', '.gif', '.mp4', '.mp3',
        'mailto:', 'tel:', 'javascript:', '#'
    ]
    
    for pattern in exclude_patterns:
        if pattern in url_lower:
            return False
    
    return True

def extract_company_name_from_url(url):
    """Extract company name from URL domain"""
    try:
        domain = urlparse(url).netloc.lower()
        
        # Remove www, subdomain prefixes
        domain = re.sub(r'^(www\.|shop\.|app\.|api\.|blog\.|news\.)', '', domain)
        
        # Get the main domain part (before TLD)
        domain_parts = domain.split('.')
        if len(domain_parts) >= 2:
            company_part = domain_parts[0]
            
            # Clean up common patterns
            company_part = re.sub(r'(health|medical|pharma|biotech|med|care)$', '', company_part)
            company_part = company_part.strip('-_')
            
            # Capitalize properly
            if company_part:
                return company_part.capitalize()
    
    except:
        pass
    
    return None

def extract_company_name_from_title(title):
    """Extract company name from page title"""
    if not title:
        return None
    
    title = clean_text(title)
    
    # Common title patterns to extract company name
    patterns = [
        # "Company Name | Tagline" or "Company Name - Tagline"
        r'^([^||\-‚Äì‚Äî]+)[\s]*[||\-‚Äì‚Äî]',
        # "Company Name: Tagline"
        r'^([^:]+):',
        # "Welcome to Company Name" 
        r'welcome\s+to\s+([^||\-‚Äì‚Äî]+)',
        # "Company Name - Official Site"
        r'^([^||\-‚Äì‚Äî]+)\s*[\-‚Äì‚Äî]\s*(official|home|website)',
        # Just take first part before common separators
        r'^([^||\-‚Äì‚Äî\(\[]+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            company_name = match.group(1).strip()
            
            # Clean up the extracted name
            company_name = clean_company_name(company_name)
            if company_name and len(company_name) > 2:
                return company_name
    
    return None

def extract_company_name_from_meta(content):
    """Extract company name from meta tags"""
    meta_patterns = [
        # og:site_name
        r'<meta[^>]*property=["\']og:site_name["\'][^>]*content=["\']([^"\']+)["\']',
        # application-name
        r'<meta[^>]*name=["\']application-name["\'][^>]*content=["\']([^"\']+)["\']',
        # author (sometimes company name)
        r'<meta[^>]*name=["\']author["\'][^>]*content=["\']([^"\']+)["\']',
        # twitter:site
        r'<meta[^>]*name=["\']twitter:site["\'][^>]*content=["\']@?([^"\']+)["\']',
    ]
    
    for pattern in meta_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            company_name = clean_text(match.group(1))
            company_name = clean_company_name(company_name)
            if company_name and len(company_name) > 2:
                return company_name
    
    return None

def clean_company_name(name):
    """Clean and normalize company name"""
    if not name:
        return None
    
    name = clean_text(name)
    
    # Remove common suffixes/prefixes
    name = re.sub(r'\s*[-‚Äì‚Äî|:]\s*(home|official|website|site).*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'^(welcome\s+to\s+)', '', name, flags=re.IGNORECASE)
    
    # Remove HTML artifacts
    name = re.sub(r'[<>]', '', name)
    
    # Clean up whitespace
    name = re.sub(r'\s+', ' ', name)
    name = name.strip()
    
    # Remove single character words at the end
    name = re.sub(r'\s+[a-zA-Z]\s*$', '', name)
    
    return name if len(name) > 2 else None

def is_generic_text(text):
    """Check if text is too generic to be a company name"""
    generic_words = [
        'home', 'about', 'contact', 'welcome', 'page', 'site', 'website',
        'official', 'login', 'register', 'search', 'menu', 'navigation',
        'privacy', 'terms', 'conditions', 'policy', 'copyright', 'reserved'
    ]
    
    text_lower = text.lower()
    return any(word in text_lower for word in generic_words)

def validate_url(url):
    """Validate URL and extract company information"""
    try:
        content, status_code = create_safe_request(url)
        
        if content and str(status_code).startswith('2'):
            # Extract title
            title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
            raw_title = title_match.group(1) if title_match else ""
            
            # Extract company name using multiple methods
            url_name = extract_company_name_from_url(url)
            title_name = extract_company_name_from_title(raw_title)
            meta_name = extract_company_name_from_meta(content)
            
            # Choose the best company name based on priority and quality
            candidates = [
                (meta_name, 'Meta tags'),
                (title_name, 'Page title'),
                (url_name, 'URL domain')
            ]
            
            # Pick first non-None candidate
            company_name = "Unknown Company"
            extraction_method = "Fallback"
            for name, method in candidates:
                if name and len(name) > 2 and not is_generic_text(name):
                    company_name = name
                    extraction_method = method
                    break
            
            # Extract description
            desc_patterns = [
                r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']',
                r'<meta[^>]*property=["\']og:description["\'][^>]*content=["\']([^"\']*)["\']',
                r'<meta[^>]*content=["\']([^"\']*)["\'][^>]*name=["\']description["\']',
            ]
            
            description = "No description available"
            for pattern in desc_patterns:
                desc_match = re.search(pattern, content, re.IGNORECASE)
                if desc_match:
                    description = clean_text(desc_match.group(1))
                    break
            
            # Determine healthcare type and country
            healthcare_type = determine_healthcare_type(url, content, raw_title)
            country = extract_country(url)
            
            # Determine source
            source = "Manual" if url in MANUAL_URLS else "Discovered"
            
            return {
                'name': company_name,
                'website': url,
                'description': description,
                'country': country,
                'healthcare_type': healthcare_type,
                'status': 'Active',
                'status_code': status_code,
                'source': source,
                'extraction_method': extraction_method,
                'raw_title': clean_text(raw_title)[:100] if raw_title else "",
                'validated_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        else:
            return create_error_record(url, f"HTTP {status_code}")
            
    except Exception as e:
        return create_error_record(url, f"Network error")

def create_error_record(url, error_msg):
    """Create error record for failed validations"""
    healthcare_type = determine_healthcare_type(url, "", "")
    country = extract_country(url)
    source = "Manual" if url in MANUAL_URLS else "Discovered"
    url_name = extract_company_name_from_url(url)
    
    return {
        'name': url_name or 'Error - Could not access',
        'website': url,
        'description': f'Error: {error_msg}',
        'country': country,
        'healthcare_type': healthcare_type,
        'status': 'Error',
        'status_code': error_msg,
        'source': source,
        'extraction_method': 'URL domain (error)',
        'raw_title': '',
        'validated_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

def clean_text(text):
    """Clean HTML and CSS from text"""
    if not text:
        return "No information available"
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Remove CSS
    text = re.sub(r'\{[^}]*\}', '', text)
    
    # Remove JavaScript
    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
    
    # Clean whitespace
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # Decode HTML entities
    html_entities = {
        '&amp;': '&', '&lt;': '<', '&gt;': '>', '&quot;': '"', '&#39;': "'",
        '&nbsp;': ' ', '&copy;': '¬©', '&reg;': '¬Æ', '&trade;': '‚Ñ¢'
    }
    
    for entity, char in html_entities.items():
        text = text.replace(entity, char)
    
    return text[:500] if len(text) > 500 else text

def determine_healthcare_type(url, content, title):
    """Determine healthcare category based on URL and content"""
    combined_text = f"{url} {content} {title}".lower()
    
    # AI/ML Healthcare
    ai_keywords = ['artificial intelligence', 'machine learning', 'ai', 'ml', 'algorithm', 'neural', 'deep learning', 'computer vision', 'nlp', 'analytics', 'data science']
    if any(keyword in combined_text for keyword in ai_keywords):
        return "AI/ML Healthcare"
    
    # Digital Health
    digital_keywords = ['telemedicine', 'telehealth', 'remote monitoring', 'mobile health', 'app', 'digital', 'online consultation', 'virtual care']
    if any(keyword in combined_text for keyword in digital_keywords):
        return "Digital Health"
    
    # Biotechnology
    biotech_keywords = ['biotech', 'biotechnology', 'pharmaceutical', 'drug development', 'clinical trials', 'biopharma', 'genetics', 'genomics']
    if any(keyword in combined_text for keyword in biotech_keywords):
        return "Biotechnology"
    
    # Medical Devices
    device_keywords = ['medical device', 'medtech', 'diagnostic equipment', 'surgical', 'imaging', 'monitoring device', 'wearable']
    if any(keyword in combined_text for keyword in device_keywords):
        return "Medical Devices"
    
    # Mental Health
    mental_keywords = ['mental health', 'psychology', 'psychiatry', 'therapy', 'counseling', 'meditation', 'mindfulness', 'depression', 'anxiety']
    if any(keyword in combined_text for keyword in mental_keywords):
        return "Mental Health"
    
    # Default
    return "Healthcare Services"

def extract_country(url):
    """Extract country from URL domain"""
    domain = urlparse(url).netloc.lower()
    
    country_map = {
        '.de': 'Germany', '.uk': 'United Kingdom', '.co.uk': 'United Kingdom',
        '.fr': 'France', '.es': 'Spain', '.it': 'Italy', '.nl': 'Netherlands',
        '.se': 'Sweden', '.dk': 'Denmark', '.no': 'Norway', '.fi': 'Finland',
        '.ch': 'Switzerland', '.at': 'Austria', '.be': 'Belgium', '.pt': 'Portugal',
        '.ie': 'Ireland', '.gr': 'Greece', '.pl': 'Poland', '.cz': 'Czech Republic',
        '.hu': 'Hungary', '.sk': 'Slovakia', '.si': 'Slovenia', '.hr': 'Croatia',
        '.bg': 'Bulgaria', '.ro': 'Romania', '.lt': 'Lithuania', '.lv': 'Latvia',
        '.ee': 'Estonia', '.lu': 'Luxembourg', '.mt': 'Malta', '.cy': 'Cyprus'
    }
    
    for tld, country in country_map.items():
        if domain.endswith(tld):
            return country
    
    return "Europe"

def save_to_files(companies, base_filename):
    """Save companies data to CSV and JSON files"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    csv_filename = f"{base_filename}_{timestamp}.csv"
    json_filename = f"{base_filename}_{timestamp}.json"
    
    # Save to CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['name', 'website', 'description', 'country', 'healthcare_type', 'status', 'status_code', 'source', 'extraction_method', 'raw_title', 'validated_date']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for company in companies:
            writer.writerow(company)
    
    # Save to JSON
    with open(json_filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(companies, jsonfile, indent=2, ensure_ascii=False)
    
    return csv_filename, json_filename

def main():
    """Main execution function"""
    print("üè• Dynamic MEGA European Healthcare Database Builder")
    print("üöÄ Using Dynamic Research Discovery + Your Manual URLs")
    print("======================================================================")
    
    # Load discovered URLs from research script
    discovered_urls = load_discovered_urls()
    
    if not discovered_urls:
        print("‚ö†Ô∏è No discovered URLs found. Using only manual URLs.")
        print("üí° Run 'python3 DYNAMIC_RESEARCH_DISCOVERY.py' first for best results.")
    
    # Combine and clean URLs
    all_urls = clean_and_deduplicate_urls(discovered_urls, MANUAL_URLS)
    
    if not all_urls:
        print("‚ùå No URLs to process. Exiting.")
        return
    
    # Validation phase
    print(f"\nüîç Starting Validation Phase...")
    print(f"üìä Total URLs to validate: {len(all_urls)}")
    print("======================================================================")
    
    companies = []
    
    for i, url in enumerate(all_urls, 1):
        print(f"[{i}/{len(all_urls)}] Validating: {url}")
        
        company_data = validate_url(url)
        companies.append(company_data)
        
        # Show progress
        status_icon = "‚úÖ" if company_data['status'] == 'Active' else "‚ùå"
        type_icon = "üîç" if 'AI/ML' in company_data['healthcare_type'] else "üìù"
        source_icon = "üìã" if company_data['source'] == 'Manual' else "üîç"
        print(f"  {status_icon}{type_icon}{source_icon} {company_data['status']} - {company_data['healthcare_type']} ({company_data['country']})")
        
        # Respectful delay
        time.sleep(1)
    
    # Save results
    csv_file, json_file = save_to_files(companies, "DYNAMIC_MEGA_EUROPEAN_HEALTHCARE_DATABASE")
    
    # Generate comprehensive report
    active_companies = [c for c in companies if c['status'] == 'Active']
    manual_companies = [c for c in companies if c['source'] == 'Manual']
    discovered_companies = [c for c in companies if c['source'] == 'Discovered']
    
    # Count by healthcare type
    type_counts = {}
    for company in active_companies:
        type_name = company['healthcare_type']
        type_counts[type_name] = type_counts.get(type_name, 0) + 1
    
    # Count by country
    country_counts = {}
    for company in active_companies:
        country_name = company['country']
        country_counts[country_name] = country_counts.get(country_name, 0) + 1
    
    # Final report
    print("\n======================================================================")
    print("üìà DYNAMIC MEGA FINAL REPORT")
    print("======================================================================")
    print("üìä OVERVIEW:")
    print(f"  ‚Ä¢ Total companies processed: {len(companies)}")
    print(f"  ‚Ä¢ Active websites: {len(active_companies)}")
    print(f"  ‚Ä¢ Manual URLs: {len(manual_companies)}")
    print(f"  ‚Ä¢ Dynamically discovered: {len(discovered_companies)}")
    print(f"  ‚Ä¢ Success rate: {(len(active_companies)/len(companies)*100):.1f}%")
    
    print(f"\nüè• HEALTHCARE CATEGORIES:")
    for healthcare_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  ‚Ä¢ {healthcare_type}: {count} companies")
    
    print(f"\nüåç COUNTRIES:")
    for country, count in sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:10]:  # Top 10
        print(f"  ‚Ä¢ {country}: {count} companies")
    
    print(f"\nüìà SOURCES:")
    print(f"  ‚Ä¢ Manual (your original): {len(manual_companies)} companies")
    print(f"  ‚Ä¢ Dynamic discovery: {len(discovered_companies)} companies")
    
    print(f"\nüíæ FILES SAVED:")
    print(f"  ‚Ä¢ {csv_file}")
    print(f"  ‚Ä¢ {json_file}")
    
    print(f"\nüéâ Dynamic MEGA Enhanced Database completed!")
    print(f"üìä {len(companies)} companies total with dynamic research integration!")
    print(f"üí° This combines your manual URLs with dynamically discovered companies!")

if __name__ == "__main__":
    main()